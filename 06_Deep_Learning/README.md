# encore_skn11

### **sk 네트웍스 famliy ai camp를 진행하면서 배우고 실습한 코드를 기록하고 정리하는 공간(Deep-Learning)**


---

- 머신러닝 VS 딥러닝 

| **구분** | **머신러닝(Machine Learning)** | **딥러닝(Deep Learning)** |
| --- | --- | --- |
| **정의** | 데이터를 학습하여 패턴을 찾고 예측하거나 분류하는 알고리즘 | 인공신경망(ANN)을 기반으로 데이터에서 고차원 특징을 학습 |
| **특징 추출** | 사람이 직접 특징을 추출해야 함 (Feature Engineering 필요) | 데이터에서 특징을 자동으로 추출 |
| **데이터 의존성** | 상대적으로 적은 데이터로도 학습 가능 | 많은 양의 데이터 필요 |
| **학습 방식** | 전통적인 알고리즘 (SVM, Decision Tree, Random Forest 등) 사용 | 딥러닝 모델(예: CNN, RNN, Transformer) 사용 |
| **처리 속도** | 일반적으로 빠르며, 가벼운 계산 | 모델이 복잡하고 계산량이 많아 GPU와 같은 고성능 HW 필요 |
| **적용 사례** | 비교적 간단한 문제 (예: 가격 예측, 스팸 분류) | 복잡한 문제 (예: 이미지/음성 인식, 자연어 처리) |
| **모델 구조** | 비교적 단순하고 해석이 용이 | 복잡한 계층 구조로 구성되어 해석이 어려움 |
| **기술 성숙도** | 오래된 기술로 안정적이고 검증됨 | 상대적으로 최근 기술, 빠르게 발전 중 |
| **연산 자원** | 일반 CPU로도 학습 가능 | GPU, TPU 같은 고성능 하드웨어 필수 |
| **오버피팅 경향** | 데이터가 적으면 오버피팅 가능성 적음 | 데이터가 부족할 경우 오버피팅 가능성 큼 |

---

- 딥러닝의 주요 알고리즘
    - CNN
        - 이미지 처리
    - RNN
        - 자연어 처리나 시계열 데이터 분석
    - GAN 
        - 생성기와 판독기
    - Transformer
        - 자연어 처리의 가장 중요

---

- 3월 17일 월요일 - 딥러닝 -퍼셉트론
    - 논리 게이트(선형)
        - AND/NAND/OR
        - 둘다, 둘다 0이여야 0, 둘중하나라고 1이면 1
    - XOR은 해결 하지 못함(비선형)
    - 가중치 : 입력에 대해 얼마나 중요한지 결정하는 값
    - 편향 : 출력 값을 조정하는 역할
    - 다층 퍼셉트론
        - 비선형 XOR 해결
    ![alt text](이미지/image.png)

---

- 3월 18일 화요일 - 딥러닝 - 활성화 함수
    - 시그모이드
        - 출력 범위 (0, 1)
        - 기울기 소실 문제 발생 가능
    ![alt text](이미지/image-1.png)
    - 계단
        - 이진분류
        - 0 이상이면 1, 이하면 0
    ![alt text](이미지/image-2.png)
    - ReLU
        - 출력 범위 (0 ~ 무한대)
        - 음수에 대해 0을 출력하여 단순 계산하여 학습 속도가 빠름
        - 죽은 뉴런 문제 발생 가능
    ![alt text](이미지/image-3.png)
    - Leaky ReLU
        - 출력 범위 (- 무한대, 무한대)
        - LeRU의 죽은 뉴런 문제를 해결하기 위해 
    ![alt text](이미지/image5.png)
    - Tanh
        - 출력범위  (-1, 1)
        - 시그모이드보다 중심이 0에 가까워 더 빠른 학습 진행 가능
        - 기울시 소실 문제 발생 가능
    ![alt text](이미지/image-4.png)

---

- 3월 19일 수요일 - 딥러닝 - 소프트맥스
    - 소프트맥스
        - 합이 1이 된다
        - 분류에 사용
    ![alt text](이미지/image6.png)

---

- 3월 20일 목요일 - 딥러닝 - 손실 함수
    - MSE
        - 실제 값과 예측값의 차이를 제곱하여 평균 낸 것 -> 제곱을 하기 떄문에 미세하게 조정
        - 이상치에 민감
    - MAE
        - 실제 값과 예측값의 차이의 절대값을 평균 낸 것 -> 그냥 차이 만큼
        - 이상치에 덜 민감 -> 미분이 어려워 최적화가 힘듬
    - Huber
        - MSE와 MAE의 장점을 합쳐 둔거
        - 오차가 작을땐 부드럽게(MSE) 오차가 크면 단호하게(MAE) 처럼
    - CEE
        - 분류 문제에서 사용
        - 실제값이 특정 클래스에 속할 확률과 예측 확률을 비교하여 손실을 계산
    - Mini-batch
        - 전체 데이터가 아닌 무작위로 선택하여 학습하는 방법
        