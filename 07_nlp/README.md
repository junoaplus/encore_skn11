# encore_skn11

### **sk 네트웍스 famliy ai camp를 진행하면서 배우고 실습한 코드를 기록하고 정리하는 공간(nlp)**

---

- 4월 2일 목요일 - nlp - 자연어 처리 개요
    - 인간이 사용하는 자연어를 컴퓨터가 이해하고 처리 할 수 있도록 하는 기술
    - nlp 접근법
        - 규칙 기반 (1950년대)
        - 통계적 기반 (1990년대)
        - 신경망 기반 (2015년대)

---

- 4월 2일 수요일 - nlp - 자연어 처리 기초
    - NLP 주요 컴포넌트
        1. 토큰화
        2. 형태소 분석
            1. 어근
            2. 접사
        3. 구분 분석
        4. 의미 분석

--- 

- 4월 3일 목요일 - nlp - nlp 전처리
    -  전처리의 단계
        1. 텍스트 정규화
            - 소문자 대문자 처리, 특수문자, 불필요 공객
        2. 토큰화
        3. 불용어 제가
            - ex) 은, 는, 이, 가
        4. 어간 추출 및 표제어 추출
            - 단어 수를 줄이거 일반화
            - ex) 하는, 했다, 한다 -> 하다
    - KoNLPy
        - 한국어 자연어 처리에 사용 되는 라이브러리 
    - 형태소 분석기 종류
        - okt
        - mecab
        - kkma

---

- 4월 7일 월요일 - nlp - 시퀀스 데이터와 RNN
    - 시퀀스 데이터란?
        - 데이터 간 순서가 의미를 가지는 데이터
        - 자연어, 음성, 시계열, 주가, 센서 데이터 등 다양하게 존재
    - RNN(Recurrent Neural Network)
        - 은닉 상태를 통해 이전 정보를 현재 입력에 반영
        - 시퀀스를 처리하는 데 적합한 구조
        - BPTT(시간에 따른 역전파)를 통해 학습
        - **단점**: 기울기 소실 문제 → 장기 기억 어려움

---

- 4월 8일 화요일 - nlp - LSTM과 GRU
    - LSTM(Long Short-Term Memory)
        - RNN의 단점을 보완한 구조
        - 셀 상태를 통해 장기 의존성 문제 해결
        - 입력/망각/출력 게이트로 구성
    - GRU(Gated Recurrent Unit)
        - LSTM보다 간단한 구조
        - 리셋 게이트와 업데이트 게이트만 존재
        - 계산량 적고 빠름, 성능 유사

| 구분 | LSTM | GRU |
|------|------|-----|
| 게이트 수 | 3개 (입력, 망각, 출력) | 2개 (업데이트, 리셋) |
| 상태 구조 | 셀 상태 + 은닉 상태 | 은닉 상태만 |
| 구조 복잡도 | 복잡함 | 단순함 |
| 계산량 | 많음 | 적음 |
| 학습 속도 | 느림 | 빠름 |

---

- 4월 9일 수요일 - nlp - Bidirectional RNN
    - 단방향 RNN은 과거 정보만 활용
    - Bidirectional RNN은 과거 + 미래 정보 모두 학습
    - 문맥 파악 성능 향상
    - 자연어 처리에서 특히 유용 (예: 품사 태깅, 개체명 인식 등)
    - 단점: 실시간 적용 어려움, 메모리 사용량 증가

---

- 4월 10일 목요일 -nlp - 텍스트 분류 모델 비교
    - 다양한 모델이 텍스트 분류에 사용됨
        - Naive Bayes: 빠르고 단순한 확률 기반 모델
        - RNN: 문맥을 반영한 시퀀스 처리
        - CNN: n-gram 기반 지역적 특징 추출
        - Multi-label: 하나의 텍스트에 여러 레이블 분류 가능

| 모델 | 특징 | 장단점 |
|------|------|--------|
| Naive Bayes | 단어 간 독립 가정 | 빠름 / 문맥 무시 |
| RNN | 순차 정보 학습 | 문맥 반영 / 느림 |
| CNN | 지역적 패턴 학습 | 빠름 / 전역 정보 부족 |
| Multi-label | 다중 클래스 분류 | 복잡한 레이블 처리 가능 |

---

- 4월 10일 목요일 - nlp - Seq2Seq
    - 기계 번역의 핵심 구조 (입력 → 출력 시퀀스 변환)
    - 인코더: 입력 시퀀스를 고정 벡터로 인코딩
    - 디코더: 고정 벡터를 바탕으로 출력 시퀀스 생성
    - Teacher Forcing: 학습 시 정답을 입력으로 사용
    - **문제점**: 고정된 컨텍스트 벡터로 정보 손실 발생 가능

---

- 4월 11일 금요일 - nlp - 어텐션 메커니즘
    - Seq2Seq의 정보 손실 문제 해결
    - 디코더가 인코더의 모든 은닉 상태를 동적으로 활용
    - 중요한 입력 단어에 집중 가능
    - 쿼리(Query), 키(Key), 값(Value) 개념 도입
    - Self-Attention → 트랜스포머 구조로 발전

| 문제 | 어텐션 해결 방식 |
|------|------------------|
| 정보 병목 | 가중치 적용으로 세밀한 정보 전달 |
| 장기 의존성 | 중요한 시점에 집중 가능 |
| 실시간 병렬 처리 | 트랜스포머에서 병렬 학습 가능 |

---

- 4월 11일 금요일 - npl - 언어 모델링
    - 언어 모델링: 다음 단어 예측 확률 학습
    - n-gram: 단순하지만 문맥 제한
    - NNLM: 임베딩 + 은닉층으로 확률 계산
    - 사전학습 모델
        - GPT: 단방향 예측 → 텍스트 생성에 유리
        - BERT: 양방향 문맥 이해 → 문장 분류 등
    - 퍼플렉서티(Perplexity): 언어 모델 평가 지표 (낮을수록 좋음)

| 모델 | 특징 |
|------|------|
| n-gram | 단기 문맥, 단순 |
| NNLM | 임베딩 기반 신경망 모델 |
| GPT | 단방향, 생성에 강점 |
| BERT | 양방향, 문맥 이해에 강점 |

