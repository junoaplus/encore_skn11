{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Word Embedding**은 단어를 고정된 차원의 백터로 변환하는 기술로, 단어 간의 의미적 유사성을 반영하도록 학습된 백터를 말한다.\n",
    "- 이 기술은 자연어 처리에서 문장을 처리하고 이해하는 데 활용된다.\n",
    "- 숫자로 표현된 단어 목록을 통해 감정을 추출하는 것도 가능하다.\n",
    "- 연관성 있는 단어 들을 군집화하여 다찬원 공간에 백더토 나타낼 수 있으며, 이는 단어나 문장을 백터 공간에 매핑하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Matrix 예시**\n",
    "\n",
    "*아래 표의 벡터 값들은 모두 기계 학습을 통해 학습된 결과이다.*  \n",
    "\n",
    "| Dimension | Man (5391) | Woman (9853) | King (4914) | Queen (7157) | Apple (456) | Orange (6257) |\n",
    "|-----------|------------|--------------|-------------|--------------|-------------|---------------|\n",
    "| 성별      | -1         | 1            | -0.95       | 0.97         | 0.00        | 0.01          |\n",
    "| 귀족      | 0.01       | 0.02         | 0.93        | 0.95         | -0.01       | 0.00          |\n",
    "| 나이      | 0.03       | 0.02         | 0.7         | 0.69         | 0.03        | -0.02         |\n",
    "| 음식      | 0.04       | 0.01         | 0.02        | 0.01         | 0.95        | 0.97          |\n",
    "\n",
    "<br>\n",
    "\n",
    "*아래는 전치된 표이다.*\n",
    "\n",
    "| Word          | 성별   | 귀족   | 나이   | 음식   |\n",
    "|---------------|--------|--------|--------|--------|\n",
    "| Man (5391)    | -1.00  | 0.01   | 0.03   | 0.04   |\n",
    "| Woman (9853)  | 1.00   | 0.02   | 0.02   | 0.01   |\n",
    "| King (4914)   | -0.95  | 0.93   | 0.70   | 0.02   |\n",
    "| Queen (7157)  | 0.97   | 0.95   | 0.69   | 0.01   |\n",
    "| Apple (456)   | 0.00   | -0.01  | 0.03   | 0.95   |\n",
    "| Orange (6257) | 0.01   | 0.00   | -0.02  | 0.97   |\n",
    "\n",
    "- **의미적 유사성 반영**  \n",
    "  - 단어를 고정된 크기의 실수 벡터로 표현하며, 비슷한 의미를 가진 단어는 벡터 공간에서 가깝게 위치한다.  \n",
    "  - 예를 들어, \"king\"과 \"queen\"은 비슷한 맥락에서 자주 사용되므로 벡터 공간에서 가까운 위치에 배치된다.  \n",
    "\n",
    "- **밀집 벡터(Dense Vector)**  \n",
    "  - BoW, DTM, TF-IDF와 달리 Word Embedding은 저차원 밀집 벡터로 변환되며, 차원이 낮으면서도 의미적으로 풍부한 정보를 담는다.  \n",
    "  - 벡터 차원은 보통 100 또는 300 정도로 제한된다.  \n",
    "\n",
    "- **문맥 정보 반영**  \n",
    "  - Word Embedding은 단어 주변의 단어들을 학습해 단어의 의미를 추론한다.  \n",
    "  - 예를 들어, \"bank\"라는 단어가 \"river\"와 함께 나오면 \"강둑\"을, \"money\"와 함께 나오면 \"은행\"을 의미한다고 학습한다.  \n",
    "\n",
    "- **학습 기반 벡터**  \n",
    "  - Word Embedding은 대규모 텍스트 데이터에서 단어 간 연관성을 학습해 벡터를 생성한다.  \n",
    "  - 반면, BoW나 TF-IDF는 단순한 규칙 기반 벡터화 방법이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소 표현(Sparse Representation) | 분산 표현(Distributed Representation)\n",
    "- 원-핫 인코딩으로 얻은 원-핫 벡터는 단어의 인덱스 값만 1이고 나머지는 모두 0으로 표현된다.\n",
    "- 이렇게 대부분의 값이 0인 벡터나 행렬을 사용하는 표현 방식을 희소 표현(sparse representation)이라고 한다.  \n",
    "- 희소 표현은 단어 벡터 간 유의미한 유사성을 표현할 수 없다는 단점이 있다.\n",
    "- 이를 해결하기 위해 단어의 의미를 다차원 공간에 벡터화하는 분산 표현(distributed representation)을 사용한다.\n",
    "- 분산 표현으로 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라고 하며, 이렇게 변환된 벡터를 임베딩 벡터(embedding vector)라고 한다.  \n",
    "- **원-핫 인코딩 → 희소 표현**  \n",
    "- **워드 임베딩 → 분산 표현**  \n",
    "\n",
    "**분산 표현(Distributed Representation)**\n",
    "- 분산 표현은 분포 가설(distributional hypothesis)에 기반한 방법이다.\n",
    "- 이 가설은 \"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\"는 내용을 전제로 한다.\n",
    "- 예를 들어, '강아지'라는 단어는 '귀엽다', '예쁘다', '애교' 등의 단어와 함께 자주 등장하며, 이를 벡터화하면 해당 단어들은 유사한 벡터값을 갖게 된다.\n",
    "- 분산 표현은 단어의 의미를 여러 차원에 걸쳐 분산하여 표현한다.  \n",
    "- 이 방식은 원-핫 벡터처럼 단어 집합 크기만큼의 차원이 필요하지 않으며, 상대적으로 저차원으로 줄어든다.\n",
    "- 예를 들어, 단어 집합 크기가 10,000이고 '강아지'의 인덱스가 4라면, 원-핫 벡터는 다음과 같다:\n",
    "  \n",
    "- **강아지 = [0 0 0 0 1 0 0 ... 0]** (뒤에 9,995개의 0 포함)  \n",
    "- 그러나 Word2Vec으로 임베딩된 벡터는 단어 집합 크기와 무관하며, 설정된 차원의 수만큼 실수값을 가진 벡터가 된다:  \n",
    "- **강아지 = [0.2 0.3 0.5 0.7 0.2 ... 0.2]**  \n",
    "\n",
    "**요약하면,**\n",
    "- 희소 표현은 고차원에서 각 차원이 분리된 방식으로 단어를 표현하지만, 분산 표현은 저차원에서 단어의 의미를 여러 차원에 분산시켜 표현한다.\n",
    "- 이를 통해 단어 벡터 간 유의미한 유사도를 계산할 수 있으며, 대표적인 학습 방법으로 Word2Vec이 사용된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Vector 시각화 wevi\n",
    "https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 2013년 구글에서 개발한 Word Embedding 방법\n",
    "- 최초의 neural embedding model\n",
    "- 매우 큰 corpus에서 자동 학습\n",
    "    - 비지도 지도 학습 (자기 지도학습)이라 할 수 있음\n",
    "    - 많은 데이터를 기반으로 label 값 유추하고 이를 지도학습에 사용\n",
    "- ex) \n",
    "    - **이사금**께 충성을 맹세하였다.\n",
    "    - **왕**께 충성을 맹세하였다.\n",
    "\n",
    "**WordVec 훈련방식에 따른 구분**\n",
    "1. CBOW : 주변 단어로 중심 단어를 예측\n",
    "2. Skip-gram : 중심 단어로 주변 단어를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words)  \n",
    "- CBOW는 원-핫 벡터를 사용하지만, 이는 단순히 위치를 가리킬 뿐 vocabulary를 직접적으로 참조하지 않는다.  \n",
    "\n",
    "**예시:**  \n",
    "\n",
    "> The fat cat sat on the mat  \n",
    "\n",
    "주어진 문장에서 'sat'이라는 단어를 예측하는 것이 CBOW의 주요 작업이다.  \n",
    "- **중심 단어(center word):** 예측하려는 단어 ('sat')  \n",
    "- **주변 단어(context word):** 예측에 사용되는 단어들  \n",
    "\n",
    "중심 단어를 예측하기 위해 앞뒤 몇 개의 단어를 참고할지 결정하는 범위를 **윈도우(window)**라고 한다.  \n",
    "예를 들어, 윈도우 크기가 2이고 중심 단어가 'sat'라면, 앞의 두 단어(fat, cat)와 뒤의 두 단어(on, the)를 입력으로 사용한다.  \n",
    "윈도우 크기가 n일 경우, 참고하는 주변 단어의 개수는 총 2n이다. 윈도우를 옆으로 이동하며 학습 데이터를 생성하는 방법을 **슬라이딩 윈도우(sliding window)**라고 한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "\n",
    "**훈련 과정**\n",
    "\n",
    "CBOW는 embedding 벡터를 학습하기 위한 구조를 갖는다. 초기에는 가중치가 임의의 값으로 설정되며, 역전파를 통해 최적화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "Word2Vec은 은닉층이 하나뿐인 얕은 신경망(shallow neural network) 구조를 사용한다.  \n",
    "학습 대상이 되는 주요 가중치는 두 가지이다:  \n",
    "\n",
    "1. **투사층(projection layer):**  \n",
    "   - 활성화 함수가 없으며 룩업 테이블 연산을 담당한다.  \n",
    "   - 입력층과 투사층 사이의 가중치 W는 V × M 행렬로 표현되며, 여기서 **V는 단어 집합의 크기, M은 벡터의 차원**이다.  \n",
    "   - W 행렬의 각 행은 학습 후 단어의 M차원 임베딩 벡터로 간주된다.  \n",
    "   - 예를 들어, 벡터 차원을 5로 설정하면 각 단어의 임베딩 벡터는 5차원이 된다.  \n",
    "\n",
    "2. **출력층:**  \n",
    "   - 투사층과 출력층 사이의 가중치 W'는 M × V 행렬로 표현된다.  \n",
    "   - 이 두 행렬(W와 W')은 서로 독립적이며, 학습 전에는 랜덤 값으로 초기화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "\n",
    "**예측 과정**\n",
    "1. CBOW는 계산된 룩업 테이블의 평균을 구한 뒤, 출력층의 가중치 W'와 내적한다.  \n",
    "2. 결과값은 **소프트맥스(softmax)** 활성화 함수에 입력되어, 중심 단어일 확률을 나타내는 예측값으로 변환된다.  \n",
    "3. 출력된 예측값(스코어 벡터)은 실제 타겟 원-핫 벡터와 비교되며, **크로스 엔트로피(cross-entropy)** 함수로 손실값을 계산한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "\n",
    "손실 함수 식:  \n",
    "$\n",
    "cost(\\hat{y}, y) = -\\sum_{j=1}^{V} y_{j} \\cdot log(\\hat{y}_{j})\n",
    "$  \n",
    "\n",
    "여기서, $\\hat{y}_{j}$는 예측 확률, $y_{j}$는 실제 값이며, V는 단어 집합의 크기를 의미한다.  \n",
    "\n",
    "\n",
    "**학습 결과**  \n",
    "- 역전파를 통해 가중치 W와 W'가 학습된다. \n",
    "- 학습이 완료되면 W 행렬의 각 행을 단어의 임베딩 벡터로 사용하거나, W와 W' 모두를 이용해 임베딩 벡터를 생성할 수 있다.  \n",
    "- CBOW는 주변 단어를 기반으로 중심 단어를 예측하는 구조를 갖추고 있으며, 이를 통해 단어 간 의미적 관계를 효과적으로 학습할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-gram\n",
    "- Skip-gram은 중심 단어에서 주변 단어를 예측한다.\n",
    "- 윈도우 크기가 2일 때, 데이터셋은 다음과 같이 구성된다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
    "\n",
    "- 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다.\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 영어 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 취득 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX\n",
      "From (redirected): https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX&confirm=t&uuid=d436364f-0217-4722-b526-cafb31bc4be0\n",
      "To: /Users/hwangjunho/Desktop/encore_skn11/07_nlp/03_word_embedding/ted_en.xml\n",
      "100%|██████████| 74.5M/74.5M [00:05<00:00, 13.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ted_en.xml'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX\"\n",
    "output = 'ted_en.xml'\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import  stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'etree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# xml 데이터 처리\u001b[39;00m\n\u001b[1;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ted_en.xml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m xml \u001b[38;5;241m=\u001b[39m \u001b[43metree\u001b[49m\u001b[38;5;241m.\u001b[39mparse(f)\n\u001b[1;32m      5\u001b[0m contens \u001b[38;5;241m=\u001b[39m xml\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//content/text()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# contens[:5]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'etree' is not defined"
     ]
    }
   ],
   "source": [
    "# xml 데이터 처리\n",
    "f = open('data/ted_en.xml', 'r', encoding='UTF-8')\n",
    "xml = etree.parse(f)\n",
    "\n",
    "contens = xml.xpath('//content/text()')\n",
    "# contens[:5]\n",
    "\n",
    "corpus = '\\n'.join(contens)\n",
    "print(len(corpus))\n",
    "\n",
    "# 정규식을 이용해 (Laughter), (Applause) 등 키워드 제거 \n",
    "corpus = re.sub(r'\\([^)]*\\)', ' ', corpus)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'reasons', 'companies', 'fail', 'new'],\n",
       " ['real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'figuring',\n",
       "  'balance',\n",
       "  'two',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'exploitation'],\n",
       " ['necessary', 'much', 'good', 'thing'],\n",
       " ['consider', 'facit'],\n",
       " ['actually', 'old', 'enough', 'remember']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리(토큰화/대소문자 정규화/불용어 처리)\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "preprocessed_sentences = []\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z0-9]', ' ', sentence)  # 영소문자, 숫자 외 제거\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    preprocessed_sentences.append(tokens)\n",
    "    \n",
    "preprocessed_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21462, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=preprocessed_sentences,   # corpus\n",
    "    vector_size=100,                    # 임베딩 백터 차원\n",
    "    sg=0,                               # 학습 알고리즘 선택 (0=CBOW, 1=Skip-gram)\n",
    "    window=5,                           # 주변 단어 수 (앞뒤로 n개 고려)\n",
    "    min_count=5                         # 최소 빈도 (빈도 n개 미만은 제거)\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>-0.914086</td>\n",
       "      <td>-0.204251</td>\n",
       "      <td>-0.380880</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.229372</td>\n",
       "      <td>-0.389345</td>\n",
       "      <td>0.483556</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>-2.527172</td>\n",
       "      <td>-0.141958</td>\n",
       "      <td>...</td>\n",
       "      <td>1.581232</td>\n",
       "      <td>0.164036</td>\n",
       "      <td>0.166712</td>\n",
       "      <td>0.090140</td>\n",
       "      <td>0.089279</td>\n",
       "      <td>-1.495349</td>\n",
       "      <td>-0.685863</td>\n",
       "      <td>-0.371761</td>\n",
       "      <td>1.203111</td>\n",
       "      <td>1.023901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>-1.848204</td>\n",
       "      <td>0.753278</td>\n",
       "      <td>0.260362</td>\n",
       "      <td>0.167711</td>\n",
       "      <td>0.211991</td>\n",
       "      <td>-1.006839</td>\n",
       "      <td>-0.369037</td>\n",
       "      <td>0.640738</td>\n",
       "      <td>-1.563160</td>\n",
       "      <td>-3.098949</td>\n",
       "      <td>...</td>\n",
       "      <td>1.184745</td>\n",
       "      <td>0.327157</td>\n",
       "      <td>-1.346133</td>\n",
       "      <td>-1.019564</td>\n",
       "      <td>-0.419322</td>\n",
       "      <td>0.442602</td>\n",
       "      <td>-1.415766</td>\n",
       "      <td>0.046096</td>\n",
       "      <td>-1.777226</td>\n",
       "      <td>1.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.786124</td>\n",
       "      <td>-0.449738</td>\n",
       "      <td>-0.985547</td>\n",
       "      <td>-1.094685</td>\n",
       "      <td>0.813575</td>\n",
       "      <td>0.102603</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.970968</td>\n",
       "      <td>-0.679625</td>\n",
       "      <td>0.994071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479482</td>\n",
       "      <td>0.633164</td>\n",
       "      <td>-0.161869</td>\n",
       "      <td>-0.182170</td>\n",
       "      <td>0.289688</td>\n",
       "      <td>0.529945</td>\n",
       "      <td>0.941778</td>\n",
       "      <td>-0.048587</td>\n",
       "      <td>0.589729</td>\n",
       "      <td>-0.438220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>-0.549405</td>\n",
       "      <td>0.093077</td>\n",
       "      <td>0.063542</td>\n",
       "      <td>-0.207110</td>\n",
       "      <td>0.028474</td>\n",
       "      <td>0.148581</td>\n",
       "      <td>-0.173005</td>\n",
       "      <td>-0.306692</td>\n",
       "      <td>-0.371393</td>\n",
       "      <td>-1.438027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509621</td>\n",
       "      <td>-0.176390</td>\n",
       "      <td>-0.405853</td>\n",
       "      <td>-0.126667</td>\n",
       "      <td>-0.368884</td>\n",
       "      <td>0.593899</td>\n",
       "      <td>0.379817</td>\n",
       "      <td>-0.573419</td>\n",
       "      <td>-0.201150</td>\n",
       "      <td>-0.183736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>-1.030696</td>\n",
       "      <td>0.844354</td>\n",
       "      <td>0.154330</td>\n",
       "      <td>-0.272247</td>\n",
       "      <td>0.733344</td>\n",
       "      <td>0.320927</td>\n",
       "      <td>-1.033069</td>\n",
       "      <td>0.660146</td>\n",
       "      <td>-1.020816</td>\n",
       "      <td>-1.055034</td>\n",
       "      <td>...</td>\n",
       "      <td>1.530719</td>\n",
       "      <td>-0.819782</td>\n",
       "      <td>-0.303733</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>-0.876845</td>\n",
       "      <td>0.630081</td>\n",
       "      <td>0.587370</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>-0.398856</td>\n",
       "      <td>0.122826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>-0.228573</td>\n",
       "      <td>0.328053</td>\n",
       "      <td>1.041662</td>\n",
       "      <td>-0.449360</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>-0.551191</td>\n",
       "      <td>0.257614</td>\n",
       "      <td>0.108930</td>\n",
       "      <td>-0.932341</td>\n",
       "      <td>-1.426811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.553141</td>\n",
       "      <td>0.544260</td>\n",
       "      <td>-0.293628</td>\n",
       "      <td>0.612608</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>-0.905314</td>\n",
       "      <td>0.072393</td>\n",
       "      <td>-0.917478</td>\n",
       "      <td>-0.142240</td>\n",
       "      <td>-0.621247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>-0.145806</td>\n",
       "      <td>0.105437</td>\n",
       "      <td>0.036838</td>\n",
       "      <td>-0.992837</td>\n",
       "      <td>-0.589101</td>\n",
       "      <td>-0.443678</td>\n",
       "      <td>-0.600631</td>\n",
       "      <td>0.361721</td>\n",
       "      <td>-1.688291</td>\n",
       "      <td>0.653416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298391</td>\n",
       "      <td>0.786892</td>\n",
       "      <td>0.068649</td>\n",
       "      <td>1.142021</td>\n",
       "      <td>0.605712</td>\n",
       "      <td>0.189012</td>\n",
       "      <td>1.235284</td>\n",
       "      <td>-0.923115</td>\n",
       "      <td>0.199129</td>\n",
       "      <td>-0.423823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.596396</td>\n",
       "      <td>0.673848</td>\n",
       "      <td>0.800010</td>\n",
       "      <td>-0.552507</td>\n",
       "      <td>1.690173</td>\n",
       "      <td>1.070844</td>\n",
       "      <td>0.239543</td>\n",
       "      <td>-0.197444</td>\n",
       "      <td>-1.581725</td>\n",
       "      <td>-0.085371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207990</td>\n",
       "      <td>-0.260511</td>\n",
       "      <td>-0.387253</td>\n",
       "      <td>1.017889</td>\n",
       "      <td>0.833557</td>\n",
       "      <td>1.416588</td>\n",
       "      <td>-0.079498</td>\n",
       "      <td>-1.041961</td>\n",
       "      <td>-1.080885</td>\n",
       "      <td>-1.457938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>-1.694257</td>\n",
       "      <td>-0.481778</td>\n",
       "      <td>0.698152</td>\n",
       "      <td>0.481112</td>\n",
       "      <td>0.735295</td>\n",
       "      <td>-0.443672</td>\n",
       "      <td>0.568155</td>\n",
       "      <td>0.777686</td>\n",
       "      <td>-0.549509</td>\n",
       "      <td>-1.517366</td>\n",
       "      <td>...</td>\n",
       "      <td>1.341535</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.553409</td>\n",
       "      <td>-0.073622</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>-0.608768</td>\n",
       "      <td>-0.831495</td>\n",
       "      <td>-0.742789</td>\n",
       "      <td>-0.342267</td>\n",
       "      <td>0.155033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>-2.373685</td>\n",
       "      <td>-1.335460</td>\n",
       "      <td>-0.197004</td>\n",
       "      <td>-0.494807</td>\n",
       "      <td>-0.185642</td>\n",
       "      <td>-0.780262</td>\n",
       "      <td>-0.838576</td>\n",
       "      <td>0.347171</td>\n",
       "      <td>-0.353197</td>\n",
       "      <td>-1.307048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218350</td>\n",
       "      <td>-0.401838</td>\n",
       "      <td>-0.632478</td>\n",
       "      <td>0.344636</td>\n",
       "      <td>0.178524</td>\n",
       "      <td>-0.200430</td>\n",
       "      <td>0.559121</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>-0.582165</td>\n",
       "      <td>0.446870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "one    -0.914086 -0.204251 -0.380880  0.058095  0.229372 -0.389345  0.483556   \n",
       "people -1.848204  0.753278  0.260362  0.167711  0.211991 -1.006839 -0.369037   \n",
       "like   -0.786124 -0.449738 -0.985547 -1.094685  0.813575  0.102603  0.004319   \n",
       "know   -0.549405  0.093077  0.063542 -0.207110  0.028474  0.148581 -0.173005   \n",
       "going  -1.030696  0.844354  0.154330 -0.272247  0.733344  0.320927 -1.033069   \n",
       "think  -0.228573  0.328053  1.041662 -0.449360  0.019508 -0.551191  0.257614   \n",
       "see    -0.145806  0.105437  0.036838 -0.992837 -0.589101 -0.443678 -0.600631   \n",
       "would   0.596396  0.673848  0.800010 -0.552507  1.690173  1.070844  0.239543   \n",
       "really -1.694257 -0.481778  0.698152  0.481112  0.735295 -0.443672  0.568155   \n",
       "get    -2.373685 -1.335460 -0.197004 -0.494807 -0.185642 -0.780262 -0.838576   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "one     0.307135 -2.527172 -0.141958  ...  1.581232  0.164036  0.166712   \n",
       "people  0.640738 -1.563160 -3.098949  ...  1.184745  0.327157 -1.346133   \n",
       "like    0.970968 -0.679625  0.994071  ... -0.479482  0.633164 -0.161869   \n",
       "know   -0.306692 -0.371393 -1.438027  ...  0.509621 -0.176390 -0.405853   \n",
       "going   0.660146 -1.020816 -1.055034  ...  1.530719 -0.819782 -0.303733   \n",
       "think   0.108930 -0.932341 -1.426811  ...  1.553141  0.544260 -0.293628   \n",
       "see     0.361721 -1.688291  0.653416  ...  0.298391  0.786892  0.068649   \n",
       "would  -0.197444 -1.581725 -0.085371  ...  0.207990 -0.260511 -0.387253   \n",
       "really  0.777686 -0.549509 -1.517366  ...  1.341535 -0.040219  0.553409   \n",
       "get     0.347171 -0.353197 -1.307048  ... -0.218350 -0.401838 -0.632478   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "one     0.090140  0.089279 -1.495349 -0.685863 -0.371761  1.203111  1.023901  \n",
       "people -1.019564 -0.419322  0.442602 -1.415766  0.046096 -1.777226  1.408248  \n",
       "like   -0.182170  0.289688  0.529945  0.941778 -0.048587  0.589729 -0.438220  \n",
       "know   -0.126667 -0.368884  0.593899  0.379817 -0.573419 -0.201150 -0.183736  \n",
       "going   1.156745 -0.876845  0.630081  0.587370  0.018001 -0.398856  0.122826  \n",
       "think   0.612608 -0.001745 -0.905314  0.072393 -0.917478 -0.142240 -0.621247  \n",
       "see     1.142021  0.605712  0.189012  1.235284 -0.923115  0.199129 -0.423823  \n",
       "would   1.017889  0.833557  1.416588 -0.079498 -1.041961 -1.080885 -1.457938  \n",
       "really -0.073622  0.107024 -0.608768 -0.831495 -0.742789 -0.342267  0.155033  \n",
       "get     0.344636  0.178524 -0.200430  0.559121  0.006409 -0.582165  0.446870  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(model.wv.vectors, index=model.wv.index_to_key).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 임베딩 모델 저장\n",
    "model.wv.save_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 로드\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "load_model = KeyedVectors.load_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9002120494842529),\n",
       " ('girl', 0.8073726892471313),\n",
       " ('daughter', 0.7850196957588196),\n",
       " ('lady', 0.7774903178215027),\n",
       " ('boy', 0.7676994800567627),\n",
       " ('father', 0.7594157457351685),\n",
       " ('son', 0.7582147717475891),\n",
       " ('grandfather', 0.7551878690719604),\n",
       " ('uncle', 0.7434059977531433),\n",
       " ('brother', 0.7430528998374939)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')\n",
    "# model.wv.most_similar('abracadabra')    # 임베딩 백터에 없는 단어로 조회 시 keyerroe 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9002120494842529),\n",
       " ('girl', 0.8073726892471313),\n",
       " ('daughter', 0.7850196957588196),\n",
       " ('lady', 0.7774903178215027),\n",
       " ('boy', 0.7676994800567627),\n",
       " ('father', 0.7594157457351685),\n",
       " ('son', 0.7582147717475891),\n",
       " ('grandfather', 0.7551878690719604),\n",
       " ('uncle', 0.7434059977531433),\n",
       " ('brother', 0.7430528998374939)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.most_similar('man')  # Word2Vec = KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69472754"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'husband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09458592, -0.06932197,  0.6821026 ,  2.1911535 ,  0.08234319,\n",
       "        0.13805117, -0.7158965 ,  1.9557717 , -1.0601429 , -1.0191189 ,\n",
       "       -0.2426851 ,  0.56587595,  0.16075015,  0.7761344 ,  0.6522487 ,\n",
       "       -0.18680926,  0.45477474,  0.2313279 , -1.5839839 ,  0.25531113,\n",
       "        0.35160244,  0.79288924,  0.41963935, -0.46848705,  0.46246758,\n",
       "       -0.0569647 , -1.0353473 , -1.0093323 , -0.13795634,  1.0061541 ,\n",
       "       -1.211307  , -1.1859783 ,  0.28122792, -1.2304024 , -0.19725353,\n",
       "        1.2616699 , -0.01559303,  0.03619621,  0.99662787, -0.5510639 ,\n",
       "        1.0439384 , -0.06821732,  0.196391  ,  0.5410864 ,  2.0331848 ,\n",
       "       -0.35567725, -0.56382746,  0.98821443,  0.6639742 , -0.80643094,\n",
       "        0.4498441 ,  0.08045097,  0.08985835, -0.23428391,  0.4727909 ,\n",
       "        0.88158184,  0.14595293,  0.5243437 , -0.32305765,  0.17023395,\n",
       "       -0.42754477, -0.7991413 , -0.76982796,  1.07764   , -1.3023582 ,\n",
       "        0.87796307, -0.09452216,  0.15141031,  1.027842  ,  1.0069293 ,\n",
       "       -0.23986687, -1.3792365 ,  0.6921051 , -1.1041334 ,  0.15923254,\n",
       "        0.6501481 ,  0.02391235,  0.25830913,  0.6737207 ,  0.22237182,\n",
       "       -1.1749847 ,  0.21368909, -0.4874674 ,  0.82764137, -0.9051395 ,\n",
       "       -0.23702118, -0.46113196,  0.08560732,  0.08975797,  0.7541736 ,\n",
       "        0.0916878 , -0.3978167 , -0.19473523, -2.2522717 ,  0.5945425 ,\n",
       "        0.22355074,  0.8165815 ,  0.24669443, -0.36887893, -0.8407359 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 시각화\n",
    "\n",
    "https://projector.tensorflow.org/\n",
    "\n",
    "- embedding vector(tensor) 파일 (.tsv)\n",
    "- metadat 파일 (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 16:12:10,123 - word2vec2tensor - INFO - running /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages/gensim/scripts/word2vec2tensor.py --input ted_en_w2v --output ted_en_w2v\n",
      "2025-04-07 16:12:10,123 - keyedvectors - INFO - loading projection weights from ted_en_w2v\n",
      "2025-04-07 16:12:10,788 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21462, 100) matrix of type float32 from ted_en_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T16:12:10.769420', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 12:55:12) [Clang 14.0.6 ]', 'platform': 'macOS-15.4-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n",
      "2025-04-07 16:12:11,465 - word2vec2tensor - INFO - 2D tensor file saved to ted_en_w2v_tensor.tsv\n",
      "2025-04-07 16:12:11,466 - word2vec2tensor - INFO - Tensor metadata file saved to ted_en_w2v_metadata.tsv\n",
      "2025-04-07 16:12:11,466 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input ted_en_w2v --output ted_en_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 Word Embedding\n",
    "- NSMC (Naver Sentiment Movie Corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from  konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('naver_movie_ratings.txt', <http.client.HTTPMessage at 0x3320d86e0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\",\n",
    "    filename=\"naver_movie_ratings.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 생성\n",
    "rating_df = pd.read_csv('naver_movie_ratings.txt' , sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    8\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rating_df.isnull().sum())\n",
    "\n",
    "rating_df = rating_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    많은 생각을 할 수 있는 영화~ 시간여행류의 스토리를 좋아하는 사람이라면 빠트릴 수...\n",
       "201    고소한 19 정말 재미있게 잘 보고 있습니다^^ 방송만 보면 털털하고 인간적이신 것...\n",
       "202                                                  가연세\n",
       "203                         goodgoodgoodgoodgoodgoodgood\n",
       "204                                           이물감. 시 같았다\n",
       "                             ...                        \n",
       "295                                   박력넘치는 스턴트 액션 평작이다!\n",
       "296                                      엄청 재미있다 명작이다 ~~\n",
       "297    나는 하정우랑 개그코드가 맞나보다 엄청 재밌게봤네요 특히 단발의사샘 장면에서 계속 ...\n",
       "298                                                적당 ㅎㅎ\n",
       "299                                    배경이 이쁘고 캐릭터도 귀엽네~\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df['document'][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글이 아닌 데이터 제거\n",
    "rating_df['document'] = rating_df['document'].replace(r'[^0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [05:33<00:00, 599.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "from tqdm import tqdm   # 진행도 시각화\n",
    "\n",
    "okt = Okt()\n",
    "ko_stopwords = ['은', '는', '이', '가', '을', '를', '와', '과', '들', '도', '부터', '까지', '에', '나', '너', '그', '걔', '얘']\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in tqdm(rating_df['document']):\n",
    "    tokens = okt.morphs(sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16841, 100)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=preprocessed_data,   # corpus\n",
    "    vector_size=100,               # 임베딩 백터 차원\n",
    "    sg=0,                          # 학습 알고리즘 선택 (0=CBOW, 1=Skip-gram)\n",
    "    window=5,                      # 주변 단어 수 (앞뒤로 n개 고려)\n",
    "    min_count=5                    # 최소 빈도 (빈도 n개 미만은 제거)\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화관', 0.9346491694450378),\n",
       " ('케이블', 0.7912479043006897),\n",
       " ('틀어주다', 0.7865746021270752),\n",
       " ('학교', 0.7728095650672913),\n",
       " ('티비', 0.7598228454589844),\n",
       " ('영화제', 0.726556658744812),\n",
       " ('방금', 0.703385591506958),\n",
       " ('개봉관', 0.6962360739707947),\n",
       " ('투니버스', 0.6727065443992615),\n",
       " ('토요명화', 0.6688910126686096)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('극장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76849544"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('김혜수', '전지현')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.wv.save_word2vec_format('naver_movie_ratings_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:10:35,621 - word2vec2tensor - INFO - running /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages/gensim/scripts/word2vec2tensor.py --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v\n",
      "2025-04-07 17:10:35,622 - keyedvectors - INFO - loading projection weights from naver_movie_ratings_w2v\n",
      "2025-04-07 17:10:36,154 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (16841, 100) matrix of type float32 from naver_movie_ratings_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T17:10:36.134114', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 12:55:12) [Clang 14.0.6 ]', 'platform': 'macOS-15.4-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n",
      "2025-04-07 17:10:36,677 - word2vec2tensor - INFO - 2D tensor file saved to naver_movie_ratings_w2v_tensor.tsv\n",
      "2025-04-07 17:10:36,678 - word2vec2tensor - INFO - Tensor metadata file saved to naver_movie_ratings_w2v_metadata.tsv\n",
      "2025-04-07 17:10:36,678 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 훈련된 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\n",
      "From (redirected): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c&confirm=t&uuid=eed5fb67-19e2-46a3-ada1-9ffc6ae46169\n",
      "To: /Users/hwangjunho/Desktop/encore_skn11/07_nlp/03_word_embedding/GoogleNews_vecs.bins.gz\n",
      "100%|██████████| 1.65G/1.65G [08:11<00:00, 3.35MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GoogleNews_vecs.bins.gz'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\"\n",
    "output = \"GoogleNews_vecs.bins.gz\"\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv = KeyedVectors.load_word2vec_format('GoogleNews_vecs.bins.gz', binary=True)\n",
    "google_news_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2294267"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similarity('king', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138046622276306),\n",
       " ('queen', 0.6510956287384033),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864824056625366),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422105193138123)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24791394"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.n_similarity(['king', 'queen'], ['man', 'woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138046622276306),\n",
       " ('queen', 0.6510956287384033),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864824056625366),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422105193138123)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similar_by_word('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.has_index_for('ㅋㅋㅋㅋㅋ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
