{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 선호에 맞는 시 창작 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환결 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (25.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (4.13.2)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (2.10.6)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (1.64.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install typing_extensions pydantic openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: peft in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: trl in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (0.40.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,DataCollatorForSeq2Seq, BitsAndBytesConfig, GenerationConfig, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import ORPOTrainer, ORPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.trainer.utils import DPODataCollatorWithPadding\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"True\"           # wandb 비활성화\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"  # 병렬 토크나이저 경고 방지\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 설정 변수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-LoRA 피인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./korean_poetry_dataset.json\"\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    poem_data = json.load(f)\n",
    "\n",
    "preprocessed_data = [{\"topic\": item[\"text\"][\"topic\"], \"poem\": item[\"text\"][\"poem\"]} for item in poem_data]\n",
    "\n",
    "train_dataset = Dataset.from_list(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 (토큰화 + labels 추가)\n",
    "def preprocess_text(sample):\n",
    "    input_text = [f\"주제: {t}\\n시: {p}\" for t, p in zip(sample[\"topic\"], sample[\"poem\"])]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        input_text, \n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    \n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model_inputs[\"labels\"] = [\n",
    "        [l if l != pad_token_id else -100 for l in labels]\n",
    "        for labels in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5fbaddab634501ad4ab6655692cd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 셋 변환\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_text, \n",
    "    batched=True,\n",
    "    remove_columns=[\"topic\", \"poem\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_quarter_precision=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages/transformers/modeling_utils.py:4228\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4225\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4228\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4235\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   4236\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pystudy_env/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 모델 훈련을 위한 준비\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.train() # 모델 학습 모드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./q_lora_peom\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    optim='adamw_bnb_8bits',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델로 시 생성\n",
    "qlora_checkpoint = \"./q_lora_peom/checkpoint-243\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(qlora_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generate_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"바람\", \"비\", \"노을\", \"달빛\", \"안개\", \"사랑\", \"이별\", \"운명\", \"기다림\", \"후회\", \"추억\", \"시간\", \"청춘\", \"변화\", \"마지막 순간\", \"군중\", \"밤거리\", \"버스\", \"인생\", \"빌딩\", \"사람들\", \"거짓말\", \"욕망\", \"돈\", \"권력\", \"비밀\", \"죽음\", \"희망\", \"동물\", \"자연\", \"도시\", \"바다\", \"산\", \"하늘\", \"별\", \"꽃\", \"나무\", \"강\", \"바위\", \"흙\", \"눈\", \"빗방울\", \"눈물\", \"웃음\"]\n",
    "\n",
    "eval_file = 'rlhf_evaluation_data.json'\n",
    "\n",
    "try:\n",
    "    with open(eval_file, 'r', encoding='utf-8') as f:\n",
    "        eval_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    eval_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 생성을 위한 변수 설정\n",
    "num_batches = 5\n",
    "batch_size = 20\n",
    "total_samples = num_batches * batch_size\n",
    "generate_samples = len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 생성 함수\n",
    "def generate_poem_batch():\n",
    "    batch_data = []\n",
    "    \n",
    "    with tqdm(total=batch_size, desc=\"<시 생성 중>\", leave=False) as t:\n",
    "        for _ in range(batch_size):\n",
    "            topic = random.choice(topics)\n",
    "            input_text = f\"주제: {topic}\\n시: \"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            poem = generate_pipeline(\n",
    "                input_text,\n",
    "                max_length=100,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "            )[0]['generated_text']\n",
    "            end_time = time.time()\n",
    "            \n",
    "            gen_time = end_time - start_time\n",
    "            \n",
    "            batch_data.append({\n",
    "                'topic': topic,\n",
    "                'poem': poem,\n",
    "                'selected': None\n",
    "            })\n",
    "            \n",
    "            # tqdm\n",
    "            t.update(1)\n",
    "            \n",
    "            global generated_samples\n",
    "            generated_samples += 1\n",
    "            complete_rate = (generated_samples / total_samples) * 100\n",
    "            remaining_time = ((total_samples - generated_samples) * gen_time) / 60\n",
    "            \n",
    "            print(f\"\\n{generated_samples} / {total_samples} 개 완료, {complete_rate:.2f}\")\n",
    "            print(f\"남은 시간: {remaining_time:.2f}분\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    return batch_data\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 저장 및 json 저장\n",
    "for _ in tqdm(range(num_batches), desc=\"<전체 진행 상황>\", position=0):\n",
    "    eval_dataset.extend(generate_poem_batch())\n",
    "    \n",
    "    with open(eval_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(eval_dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward model\n",
    "- 앞서 생성한 시에 대해서 selected=true로 수정해 피드백 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 시+선호도 파일 로드\n",
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "    \n",
    "    \n",
    "reward_data = [\n",
    "    {'text_a': f\"주제: {item[\"topic\"]}\", 'text_b': item[\"poem\"]}\n",
    "    for item in evaluation_data if item[\"selected\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 (베치 데이터 처리)\n",
    "def preprocess_reward_data(sample):\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        sample[\"text_a\"],\n",
    "        text_pair=sample[\"text_b\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    \n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model_inputs[\"labels\"] = [\n",
    "        [l if l != pad_token_id else -100 for l in labels]\n",
    "        for labels in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "reward_dataset = reward_dataset.map(\n",
    "    preprocess_reward_data,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text_a\", \"text_b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 생성\n",
    "reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "reward_model = prepare_model_for_kbit_training(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 적용\n",
    "reward_model = get_peft_model(reward_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer 설정 \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./reward_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "reward_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=reward_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLHF (ORPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(qlora_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "# 파라미터 학습 가능하게끔 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch CUDA 메모리 관리 최적화 (OOM 방지)\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO 데이터셋 준비\n",
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "    \n",
    "orpo_data = []\n",
    "\n",
    "for item in evaluation_data:\n",
    "    if item[\"selected\"]:\n",
    "        prompt_text = f\"주제: {item['topic']}\\n이 주제에 맞는 시를 작성해 주세요.\"\n",
    "        chosen_text = item[\"poem\"]\n",
    "        rejected_text = \"\"\n",
    "        \n",
    "        tokenized_prompt = tokenizer(prompt_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        tokenized_chosen = tokenizer(chosen_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        tokenized_rejected = tokenizer(rejected_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        \n",
    "        orpo_data.append({\n",
    "            \"prompt\": prompt_text,\n",
    "            \"chosen\": chosen_text,\n",
    "            \"rejected\": rejected_text,\n",
    "            \"prompt_input_ids\": tokenized_prompt[\"input_ids\"].squeeze(0).cuda(),\n",
    "            \"prompt_attention_mask\": tokenized_prompt[\"attention_mask\"].squeeze(0).cuda(),\n",
    "            \"chosen_input_ids\": tokenized_chosen[\"input_ids\"].squeeze(0).cuda(),\n",
    "            \"chosen_attention_mask\": tokenized_chosen[\"attention_mask\"].squeeze(0).cuda(),\n",
    "            \"rejected_input_ids\": tokenized_rejected[\"input_ids\"].squeeze(0).cuda(),\n",
    "            \"rejected_attention_mask\": tokenized_rejected[\"attention_mask\"].squeeze(0).cuda()\n",
    "        })\n",
    "        \n",
    "        orpo_dataset = Dataset.from_list(orpo_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO 설정\n",
    "from numpy import gradient\n",
    "\n",
    "\n",
    "orpo_config = ORPOConfig(\n",
    "    output_dir=\"./orpo_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e6,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=100,\n",
    "    save_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 콜레이터 설정\n",
    "data_colloator = DPODataCollatorWithPadding(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    label_pad_token_id = -100,\n",
    "    is_encoder_decoder=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPOTrainer 초기화\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_config,\n",
    "    data_colloator=data_colloator,\n",
    "    train_dataset=orpo_dataset,\n",
    "    process_function=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "orpo_trainer.train()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orpr_checkpoint = './orpo_output/checkpoint-xxx'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(orpr_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generate_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_peom_final(num_samples=5):\n",
    "    topics = [\"바람\", \"비\", \"노을\", \"달빛\", \"안개\", \"사랑\", \"이별\", \"운명\", \"기다림\", \"후회\", \"추억\", \"시간\", \"청춘\", \"변화\", \"마지막 순간\", \"군중\", \"밤거리\", \"버스\", \"인생\", \"빌딩\", \"사람들\", \"거짓말\", \"욕망\", \"돈\", \"권력\", \"비밀\", \"죽음\", \"희망\", \"동물\", \"자연\", \"도시\", \"바다\", \"산\", \"하늘\", \"별\", \"꽃\", \"나무\", \"강\", \"바위\", \"흙\", \"눈\", \"빗방울\", \"눈물\", \"웃음\"]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        topic = random.choice(topics)\n",
    "        input_text = f\"주제: {topic}\\n 시\"\n",
    "        poem = generate_poem(\n",
    "            input_text,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        results.append({\n",
    "            \"topic\": topic,\n",
    "            \"poem\": poem\n",
    "        })\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_poem = generate_peom_final(num_samples=10)\n",
    "generated_poem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
