{"cells":[{"cell_type":"markdown","metadata":{"id":"_dcFj6uZRCt5"},"source":["# Mediapipe"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"WRxcD_1sRCt8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (4.11.0.86)\n","Requirement already satisfied: mediapipe in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (0.10.21)\n","Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from opencv-python) (1.26.4)\n","Requirement already satisfied: absl-py in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (2.2.2)\n","Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (25.2.10)\n","Requirement already satisfied: jax in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (0.4.30)\n","Requirement already satisfied: jaxlib in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (0.4.30)\n","Requirement already satisfied: matplotlib in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (3.9.4)\n","Requirement already satisfied: opencv-contrib-python in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (4.11.0.86)\n","Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (4.25.7)\n","Requirement already satisfied: sounddevice>=0.4.4 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (0.5.1)\n","Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from mediapipe) (0.2.0)\n","Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: pycparser in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from jax->mediapipe) (0.5.1)\n","Requirement already satisfied: opt-einsum in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from jax->mediapipe) (3.3.0)\n","Requirement already satisfied: scipy>=1.9 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from jax->mediapipe) (1.13.1)\n","Requirement already satisfied: importlib-metadata>=4.6 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from jax->mediapipe) (8.6.1)\n","Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.21.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (24.2)\n","Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from matplotlib->mediapipe) (6.5.2)\n","Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/vectordb_env/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install opencv-python mediapipe"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QzV2dSgjRCt9"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1747100254.645038  422924 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n","INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","W0000 00:00:1747100254.646525  423324 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"]}],"source":["import cv2\n","import mediapipe as mp\n","\n","# 얼굴 검출 모델 초기화\n","mp_face_detector = mp.solutions.face_detection\n","mp_drawing = mp.solutions.drawing_utils\n","\n","face_detector = mp_face_detector.FaceDetection(model_selection=0, min_detection_confidence=0.5)"]},{"cell_type":"markdown","metadata":{"id":"yJSSpAPKRCt-"},"source":["### 1. 얼굴(눈,코,입,귀) 인식"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QAfkHtceRCt_"},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","\n","    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","    results = face_detector.process(img_rgb)\n","\n","    # print(results.detections)\n","\n","    if results.detections:\n","        for detection in results.detections:\n","            mp_drawing.draw_detection(frame, detection)\n","\n","    cv2.imshow('Face Detection', frame)\n","\n","    if cv2.waitKey(1) & 0xFF == 27:\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"0v6CDqBLRCt_"},"source":["### 2. 얼굴 인식 모자이크"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Sp6__BxlRCuA"},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","\n","    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    img_h, img_w, _ = img_rgb.shape\n","\n","    results = face_detector.process(img_rgb)\n","\n","    if results.detections:\n","        for detection in results.detections:\n","            bbox = detection.location_data.relative_bounding_box\n","\n","            x1 = int(max(bbox.xmin, 0) * img_w)\n","            y1 = int(max(bbox.ymin, 0) * img_h)\n","            width = int(bbox.width * img_w)\n","            height = int(bbox.height * img_h)\n","            x2 = min(x1 + width, img_w)\n","            y2 = min(y1 + height, img_h)\n","\n","            face = img_rgb[y1:y2, x1:x2]\n","            face = cv2.resize(face, (10, 10), interpolation=cv2.INTER_LINEAR)\n","            face = cv2.resize(face, (x2-x1, y2-y1), interpolation=cv2.INTER_LINEAR)\n","\n","            img_rgb[y1:y2, x1:x2] = face\n","\n","    img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n","    cv2.imshow('Face Detection', img_rgb)\n","\n","    if cv2.waitKey(1) & 0xFF == 27:\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"MIolfdUPRCuA"},"source":["### 3. FaceMesh"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"EcbDnGUDRCuB"},"outputs":[],"source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","\n","# 얼굴 인식 및 얼굴 메쉬 솔루션 초기화\n","mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","mp_face_mesh = mp.solutions.face_mesh"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fwZOP6dHRCuB"},"outputs":[{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747100333.672153  422924 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n","W0000 00:00:1747100333.674752  429034 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","W0000 00:00:1747100333.682774  429040 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","W0000 00:00:1747100333.773529  429034 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"]}],"source":["cap = cv2.VideoCapture(0)\n","\n","with mp_face_mesh.FaceMesh(\n","    max_num_faces=1,\n","    refine_landmarks=True,\n","    min_detection_confidence=0.5,\n","    min_tracking_confidence=0.5\n",") as face_mesh:\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","\n","        if not ret:\n","            break\n","\n","        frame = cv2.flip(frame, 1)\n","        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","        image.flags.writeable = False\n","        results = face_mesh.process(image)\n","        image.flags.writeable = True\n","\n","        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","        if results.multi_face_landmarks:\n","            for face_landmark in results.multi_face_landmarks:\n","\n","                # 얼굴 3D 구조 시각화 (테셀레이션)\n","                mp_drawing.draw_landmarks(\n","                    image=image,\n","                    landmark_list=face_landmark,\n","                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n","                    landmark_drawing_spec=None,\n","                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n","                )\n","\n","                # 얼굴 윤곽선과 주요 특징 강조 (윤곽선, 눈썹, 눈, 입술)\n","                mp_drawing.draw_landmarks(\n","                    image=image,\n","                    landmark_list=face_landmark,\n","                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n","                    landmark_drawing_spec=None,\n","                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()\n","                )\n","\n","                # 눈의 형태와 위치 강조\n","                mp_drawing.draw_landmarks(\n","                    image=image,\n","                    landmark_list=face_landmark,\n","                    connections=mp_face_mesh.FACEMESH_IRISES,\n","                    landmark_drawing_spec=None,\n","                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()\n","                )\n","\n","        cv2.imshow('Face Mesh', image)\n","\n","        if cv2.waitKey(1) & 0xFF == 27:\n","            break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"e6yEmw0PRCuB"},"source":["### 4. 필터  씌우기"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"DleIHBt6RCuC"},"outputs":[{"name":"stderr","output_type":"stream","text":["I0000 00:00:1747100405.281247  422924 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n","W0000 00:00:1747100405.283678  434026 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1747100405.289167  434026 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n","\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n","\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n","\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."]}],"source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","\n","mp_draw = mp.solutions.drawing_utils\n","mp_face_mesh = mp.solutions.face_mesh\n","\n","face_mesh = mp_face_mesh.FaceMesh()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Folha4bFRCuC"},"outputs":[{"name":"stderr","output_type":"stream","text":["[ WARN:0@112.632] global loadsave.cpp:268 findDecoder imread_('mini.png'): can't open/read file: check file path/integrity\n"]}],"source":["mask_img = cv2.imread('mini.png', cv2.IMREAD_UNCHANGED)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"UIB0NSFoRCuC"},"outputs":[{"name":"stderr","output_type":"stream","text":["W0000 00:00:1747100366.815802  431373 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"]}],"source":["# 얼굴 위에 오버레이 이미지를 합성하는 함수\n","def face_overlay(background_img, overlay_img, x, y, overlay_size=None):\n","    try:\n","        bg_img = background_img.copy()\n","\n","        if bg_img.shape[2] == 3:\n","            bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)\n","\n","        if overlay_size is not None:\n","            overlay_img = cv2.resize(overlay_img.copy(), overlay_size)\n","\n","        b, g, r, a = cv2.split(overlay_img)\n","        mask = cv2.medianBlur(a, 5)\n","\n","        h, w, _ = overlay_img.shape\n","        roi = bg_img[int(y - h/2):int(y + h/2), int(x - w/2):int(x + w/2)]\n","        img_bg1 = cv2.bitwise_and(roi, roi, mask=cv2.bitwise_not(mask))\n","        img_bg2 = cv2.bitwise_and(overlay_img.copy(), overlay_img.copy(), mask=mask)\n","        bg_img[int(y - h/2):int(y + h/2), int(x - w/2):int(x + w/2)] = cv2.add(img_bg1, img_bg2)\n","\n","        bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)\n","        return bg_img\n","\n","    except:\n","        return background_img"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ODmhKojURCuC"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m         face_result \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m     36\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini mouse\u001b[39m\u001b[38;5;124m'\u001b[39m, face_result)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     41\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, img = cap.read()\n","\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    result = face_mesh.process(img_rgb)\n","    img_h, img_w, img_c = img.shape\n","\n","    if result.multi_face_landmarks:\n","        for face_landmark in result.multi_face_landmarks:\n","            xy_point = []\n","\n","            for c, lm in enumerate(face_landmark.landmark):\n","                xy_point.append([lm.x, lm.y])\n","                img = cv2.circle(img, (int(lm.x * img_w), int(lm.y * img_h)), 1, (255,0,0), 2)\n","\n","            top_left = np.min(xy_point, axis=0)\n","            bottom_right = np.max(xy_point, axis=0)\n","            mean_xy = np.mean(xy_point, axis=0)\n","            img = cv2.circle(img, (int(mean_xy[0] * img_w), int(mean_xy[1] * img_h)), 4, (0,0,255), 3)\n","            face_width = int(bottom_right[0] * img_w) - int(top_left[0] * img_w)\n","            face_height = int(bottom_right[1] * img_h) - int(top_left[1] * img_h)\n","\n","            if face_width > 0 and face_height > 0:\n","                face_result = face_overlay(\n","                                    img,\n","                                    mask_img,\n","                                    int(mean_xy[0] * img_w),\n","                                    int(mean_xy[1] * img_h),\n","                                    overlay_size=(face_width, face_height)\n","                                )\n","    else:\n","        face_result = img\n","\n","    cv2.imshow('mini mouse', face_result)\n","\n","    if cv2.waitKey(1) == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"vectordb_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}
